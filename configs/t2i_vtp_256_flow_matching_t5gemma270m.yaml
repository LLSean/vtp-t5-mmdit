experiment_name: t2i_vtp_fm_256_t5gemma270m
output_dir: runs
seed: 42

video:
  width: 256
  height: 256
  fps: 16
  num_frames: 1
  frame_stride: 1

tokenizer:
  type: vtp
  name_or_path: ./VTP-Small-f16d64
  repo_path: ./VTP
  latent_f: 16
  latent_d: 64

text_encoder:
  type: hf
  # * NOTE: this model is gated; you must accept the license on HF and have a token available.
  name_or_path: ./t5gemma-2-270m-270m
  max_length: 128
  freeze: true
  use_processor: true
  trust_remote_code: false

model:
  dim: 512
  depth: 8
  num_heads: 8
  window_size: 8
  mlp_ratio: 4.0
  dropout: 0.0

flow_matching:
  path: linear
  t_min: 0.0
  t_max: 1.0
  time_scale: 999.0

diffusion:
  num_timesteps: 1000
  schedule: cosine
  prediction_type: eps

train:
  objective: flow_matching
  precision: bf16
  batch_size: 32
  grad_accum_steps: 1
  lr: 0.0001
  weight_decay: 0.0
  max_steps: 10000
  log_every_steps: 50
  save_every_steps: 500
  cfg_dropout_prob: 0.1

  sample_every_steps: 500
  sample_num_images: 4
  sample_num_steps: 50
  sample_solver: euler
  sample_cfg_scale: 5.0
  sample_seed: 123
  sample_save_denoise_debug: true
  sample_denoise_t: 0.9

lora:
  enabled: false
  rank: 16
  alpha: 16
  dropout: 0.0
  train_lora_only: true
  target_modules: [q, k, v, proj]


